{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.vision.gan import *\n",
    "from torchvision.models import vgg16_bn\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torchvision.models as tv_models\n",
    "import pathlib\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def gram_matrix(x):\n",
    "    n,c,h,w = x.size()\n",
    "    x = x.view(n, c, -1)\n",
    "    return (x @ x.transpose(1,2))/(c*h*w)\n",
    "\n",
    "base_loss = F.l1_loss\n",
    "\n",
    "class FeatureLoss(nn.Module):\n",
    "    def __init__(self, m_feat, layer_ids, layer_wgts):\n",
    "        super().__init__()\n",
    "        self.m_feat = m_feat\n",
    "        self.loss_features = [self.m_feat[i] for i in layer_ids]\n",
    "        self.hooks = hook_outputs(self.loss_features, detach=False)\n",
    "        self.wgts = layer_wgts\n",
    "        self.metric_names = ['pixel',] + [f'feat_{i}' for i in range(len(layer_ids))\n",
    "              ] + [f'gram_{i}' for i in range(len(layer_ids))]\n",
    "\n",
    "    def make_features(self, x, clone=False):\n",
    "        self.m_feat(x)\n",
    "        return [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        out_feat = self.make_features(target, clone=True)\n",
    "        in_feat = self.make_features(input)\n",
    "        self.feat_losses = [base_loss(input,target)]\n",
    "        self.feat_losses += [base_loss(f_in, f_out)*w\n",
    "                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n",
    "        self.feat_losses += [base_loss(gram_matrix(f_in), gram_matrix(f_out))*w**2 * 5e3\n",
    "                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n",
    "        self.metrics = dict(zip(self.metric_names, self.feat_losses))\n",
    "        return sum(self.feat_losses)\n",
    "\n",
    "    def __del__(self): self.hooks.remove()\n",
    "\n",
    "        \n",
    "def create_data(path_to_source, path_to_target, bs, size, tfms, test_share=0.05):\n",
    "    src = ImageImageList.from_folder(path_to_source).split_by_rand_pct(test_share, seed=42)\n",
    "    data = (src.label_from_func(lambda x: path_to_target / x.name).transform(tfms, size=size, tfm_y=True)\n",
    "            .databunch(bs=bs).normalize(imagenet_stats, do_y=True))\n",
    "    data.c = 3\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_crit_data(ds_path, classes, bs, size, test_share=0.05):\n",
    "    src = ImageList.from_folder(ds_path, include=classes).split_by_rand_pct(test_share, seed=42)\n",
    "    ll = src.label_from_folder(classes=classes)\n",
    "    data = (ll.transform(get_transforms(do_flip=True, max_rotate=45, max_zoom=4., max_warp=0.2), size=size)\n",
    "           .databunch(bs=bs).normalize(imagenet_stats))\n",
    "    data.c = 3\n",
    "    return data\n",
    "\n",
    "def create_gen_learner(data_gen, arch, wd, y_range, loss_gen):\n",
    "    return unet_learner(data_gen, arch, wd=wd, blur=True, norm_type=NormType.Weight,\n",
    "                         self_attention=True, y_range=y_range, loss_func=loss_gen)\n",
    "\n",
    "def create_critic_learner(data, wd, loss_func, metrics):\n",
    "    return Learner(data, gan_critic(), metrics=metrics, loss_func=loss_func, wd=wd)\n",
    "\n",
    "def save_preds(dl):\n",
    "    i=0\n",
    "    names = dl.dataset.items\n",
    "    \n",
    "    for b in dl:\n",
    "        preds = learn_gen.pred_batch(batch=b, reconstruct=True)\n",
    "        for o in preds:\n",
    "            o.save(path_gen/names[i].name)\n",
    "            i += 1\n",
    "       \n",
    "def num_params(model):\n",
    "    return sum([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to your dataset here\n",
    "DS_PATH = pathlib.Path(\"path/to/dataset/\")\n",
    "# path to source domain\n",
    "DIR_A = \"trainA\"\n",
    "PATH_A = DS_PATH / DIR_A\n",
    "# path to target domain\n",
    "DIR_B = \"trainB\"\n",
    "PATH_B = DS_PATH / DIR_B\n",
    "# prefix for saved pretrained generator, critic and final generator weights\n",
    "SAVE_PREFIX = \"my-custom-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "IMG_SIZE = 256\n",
    "ENCODER_ARCH = models.resnet34\n",
    "print(f\"Encoder #params: {num_params(ENCODER_ARCH()) / 1e6} M\")\n",
    "TRAIN_TFMS = get_transforms(do_flip=True, max_rotate=45, max_zoom=4., max_warp=0.2)\n",
    "TEST_SHARE = 0.05\n",
    "\n",
    "WEIGHT_DECAY = 1e-3\n",
    "Y_RANGE = (-3, 3)\n",
    "\n",
    "# pretrain encoder only for this many epochs\n",
    "NUM_EPOCHS_PRETRAIN_ENCODER = 2\n",
    "# pretrain whole generator for this many more epochs\n",
    "NUM_EPOCHS_PRETRAIN_WHOLE_GEN = 3\n",
    "# pretrain whole generator with this lr\n",
    "LR_PRETRAIN_WHOLE_GEN = slice(1e-6, 1e-3)\n",
    "# pretrain critic for this many epochs\n",
    "NUM_EPOCHS_PRETRAIN_CRIT = 12\n",
    "# pretrain critic with this lr\n",
    "LR_PRETRAIN_CRIT = 1e-3\n",
    "\n",
    "# train entire GAN for this many epochs\n",
    "NUM_EPOCHS_TRAIN_GAN = 50\n",
    "# train entire GAN with this lr\n",
    "LR_TRAIN_GAN = 1e-4\n",
    "CRITIC_THRESHOLD = 0.65\n",
    "\n",
    "# Feature loss, can be modified or left as is\n",
    "vgg_m = vgg16_bn(True).features.to(DEVICE).eval()\n",
    "requires_grad(vgg_m, False)\n",
    "blocks = [i - 1 for i, o in enumerate(children(vgg_m)) if isinstance(o, nn.MaxPool2d)]\n",
    "FEAT_LOSS = FeatureLoss(vgg_m, blocks[2:5], [5, 15, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Pre-train the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = ImageImageList.from_folder(PATH_A).split_by_rand_pct(TEST_SHARE, seed=42)\n",
    "data_gen = create_data(PATH_A, PATH_B, BATCH_SIZE, IMG_SIZE, TRAIN_TFMS, TEST_SHARE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_gen.show_batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_gen = create_gen_learner(data_gen, ENCODER_ARCH, wd=WEIGHT_DECAY, y_range=Y_RANGE, loss_gen=FEAT_LOSS)\n",
    "print(f\"Generator #params: {num_params(learn_gen.model) / 1e6} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_gen.fit_one_cycle(NUM_EPOCHS_PRETRAIN_ENCODER, pct_start=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_gen.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_gen.fit_one_cycle(NUM_EPOCHS_PRETRAIN_WHOLE_GEN, LR_PRETRAIN_WHOLE_GEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_gen.show_results(rows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_gen.save(SAVE_PREFIX + \"-pretrained-gen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make generated images for critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_gen.load(SAVE_PREFIX + \"-pretrained-gen\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_gen = SAVE_PREFIX + \"_images_gen\"\n",
    "path_gen = DS_PATH / name_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if already exists needed\n",
    "# shutil.rmtree(path_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_gen.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_preds(data_gen.fix_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.open(path_gen.ls()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_gen=None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crit = create_crit_data(DS_PATH, classes=[name_gen, DIR_B],\n",
    "                             bs=BATCH_SIZE, size=IMG_SIZE, test_share=TEST_SHARE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crit.show_batch(rows=3, ds_type=DatasetType.Train, imgsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_critic = AdaptiveLoss(nn.BCEWithLogitsLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_critic = create_critic_learner(data_crit, wd=WEIGHT_DECAY, loss_func=loss_critic, metrics=accuracy_thresh_expand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_critic.fit_one_cycle(NUM_EPOCHS_PRETRAIN_CRIT, LR_PRETRAIN_CRIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_critic.save(SAVE_PREFIX + \"-pretrained-crit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_critic=None\n",
    "learn_gen=None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crit = create_crit_data(DS_PATH, [DIR_A, DIR_B], bs=BATCH_SIZE, size=IMG_SIZE, test_share=TEST_SHARE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_crit = create_critic_learner(data_crit, wd=WEIGHT_DECAY,\n",
    "                                   loss_func=loss_critic, metrics=None).load(SAVE_PREFIX + \"-pretrained-crit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_gen = create_gen_learner(data_gen, ENCODER_ARCH, WEIGHT_DECAY, Y_RANGE, FEAT_LOSS).load(SAVE_PREFIX + \"-pretrained-gen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "switcher = partial(AdaptiveGANSwitcher, critic_thresh=CRITIC_THRESHOLD)\n",
    "learn = GANLearner.from_learners(learn_gen, learn_crit, weights_gen=(1.,50.), show_img=True, switcher=switcher,\n",
    "                                 opt_func=partial(optim.Adam, betas=(0.,0.99)), wd=WEIGHT_DECAY)\n",
    "learn.callback_fns.append(partial(GANDiscriminativeLR, mult_lr=5.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(NUM_EPOCHS_TRAIN_GAN, LR_TRAIN_GAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(SAVE_PREFIX + \"-trained-gan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn for more if needed\n",
    "# learn.fit(10, lr / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn_gen.show_results(rows=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learn_gen.model.state_dict(), DS_PATH / \"models\" / (SAVE_PREFIX + \"-gen-state-dict.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai_v1",
   "language": "python",
   "name": "fastai_v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
